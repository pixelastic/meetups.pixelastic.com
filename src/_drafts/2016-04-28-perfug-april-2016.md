---
layout: post
title: "perfUG April 2016"
tags: perfUG
---

31. almost 3 years

Latency: from a dream to nightmare in 100ms

room almost full

almost all room knew us

normalement, tout est dans le même server,/dataenter
1ms latency dans le même datacenter, même rack
si plus de 1, c'est que c'est tres loin. 1ms speed of light, 300km

paris-new york
39ms, distence, speed of light 39ms
reality 85ms
pas de math simple, pas que la vitesse de la lumirere
plus que 2x les estimations

133ms est la latence théorique maximum sur la planete
paris sydney 322ms, why? not shortest plan, delays, thngs in between

millisecondsmatter
search as you type
each character is a request
the faster it is, the happier the user
+100ms (Amazon, 1% revenue)

data replicated 3 times, on three servers in the datacenter
of huge reliance, we deploy in three providers on three separate datacenters
même si ona  un clusyer avec troi mashinces dans des privuders différent,s il
sont encore géographiquement prhcs et ca reste autour de 1ms

mais quand splitted dans le monde, on touche des ms de 300ms environ
pose des soucis de sync

scale
3x data
15 regions, 35 datacenter, 1900 links monitored (inside cluster, outside)
400+ servers, 12B+ queries per month

no optimize latency, does not care
doesn not care about geography (politics, and money)
optimisé pour la vidéo, grosse bandwidth, on peu buffer, mais on se moque de la
latency
on motigate avec un CDN pour le mettre plus proche
mais marche pas pour du dnamique, marche que pour du cache

il y a des networks privés pour le low latency
backbone pour stock trading, high frewuency, low latency. milliseconds change if
you are in the deal or not
Google has its own network, with borders close to you
edge close to end user, fastest way between datacenters, private fiber lines
inside

does not follow geography, follow money and politics
paris-singapor
from office, or from datacenter is different route
160s, marseille, mumbai, chennai, singapore
we are used that connection from home is bad, and datacenter is better

from datacenter, different path
passe par les US, 257ms au lieu de 160ms
depart/arrivé identique, chemin different

guideline pour les serveur name
manufacturer, speed, location from the server name
marking by IATA codes, codes of airport


japan, cluster in two datacenters, tokyo and osaka
more resilient
8ms in average
one day, 110ms, break consensus
osaka tokyo test, 100ms
toujours tester les deux cotés de la connection
TOUJOURS, résultats différentes A->B et B->A
link tokyo->osaka was not by their line, but their own provider
send email, prove that you're impacted, escalate to network team
issue resolved by moving to another link
but they are not monitoring it
most customers us DC to link to internet, not to another one close

get the address of DC by looking at job pages

rerouting par un link qui passe par l'autre coté des US, mais en perdant 20% des
data
mais comme TCP, peu recover de la perte de data, en renvoyant les paquets plus
petits, finit par arriver mais forte mlatence

demande à AWS et autre provider, les deux disent que pas leur faute
le probleme vient de leur provider, demnde de contacter mais no, pas nous le
client
donc go support, livechat on amazon is great

pas pu le fixer, donc on reroute par un autre datacenter pour esperer passer par
un autre endroit
toutes les availability zone vont vers e même router, donc change rien
outside of your network, outside network of destination, in between, who contact
who to resolve it? And no one monitor or care

Ashburn
Main region of amazon. Most of the web hosted there
2 providers, spread in 2 datacenters
Provider A, 2ms to amazon
Provider B, 8ms to amazon
same location, same destination

Ashburn/Ashburn should be low latency, but goes through NY
no one cares, rejetent la faute, tous les deux convaincus
escalated to netwokr team "6ms, we don't care"
"prove that 6ms makes a business impact"
8 month of ping pong with support
error on configuration in AWS

"Looking glass" provided by main providers, permet de voir la configuration des
routers. qui voit quel route, priorités, qui voit quoi
patience, persuasive, talk the language

need independent monitoring of your network
monitor latency to external services/from external services
pingdom, pings from location on regular basis
ThousandEyes awesome. performance of service, page load, dns, establish ssl
connection, etc. from all locations. But expensive for multiple destnations

TurboBytes, community traceroute rom everywhere, even from inside china

custom probes
ruby script on small VPS, send checks constantly, aggregate in logs
must monitor DNS, if DNS provider fails, everything fails
avec des probes partout, on sait si une issue vient du customer ou si plus
général

status page automatique depuis les probes
mets à jour le statusautomaticuement avant de pager duty les ops

status page officielles ont 4 status. Green, Green "with an eye", orange and
red.
you never see red. green is normal
green with an eye means it's broken "elevater error rate"
orange means "broken and customers noticed"

can get the latency tehcnically from the time to get the ssl handshake
ideas came from 2001
tcp info can be get from nginx, can change the log format and get the latency






great speaker, really funny stories

