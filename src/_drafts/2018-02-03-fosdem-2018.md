---
layout: post
title: "FOSDEM 2018"
tags:
---

On this first week-end of February was my first time going to the FOSDEM (Free
and Open-Source Developer European Meetup). This took place in a university
campus in Brussels, and is one of the biggest tech event I ever attended.

The event is entirely free, and I highly underestimated the number of people
that would show up. Talks are non-stop all day long and rooms fill up quickly.
I learned the hard way that if you have a good spot in a room, you should not
leave to try to see another talks. I ended up not being able to enter into the
room I was aiming for, and not able to get back to the one I just left.

The strategy that worked best for me was to pick one track room and stay there
for a 3 to 4 hours, watching all the talks of the track. I did manage to see
a few other talks outside of my chosen tracks, but mostly I spend my time in the
*Mozilla* and *Tool the docs* rooms. Also, not all rooms are equal and some
makes taking notes on a laptop quite an exercise in gymnastics.

## Mozilla room

Talks I saw in this room were mostly focused around Firefox, and specifically
the 58 release and its performance improvements.

The build pipeline for Firefox is impressive in terms of linting and testing
that is integrated into the CI, and especially the number of platforms they are
running the tests on. They tried to automate as much as the could to catch as
much issues as possible as soon as possible.

This includes capturing any error stacktrace and automatically checking the
files it goes through to check if those files were not part of a recent commit.
If it does, it notifies the people that changed those files recently, that might
have an insight about where the bug is coming from.

They also do some fuzzying in their tests. This consists of sending random input
data to the app and see if and how it crashes. Allowing to catch corner cases
bugs automatically.

They also measure performance of every part of the app, by checking the time
each part takes to execute. If a part decreases too much, the build will error,
and they will have the opportunity to fix it. In their experience, performance
loss is always gradual, and in such small steps that it is virtually impossible
to detect if not done automatically.

Performance is also mostly a matter of perception. It is more important that
things look fast than them being actually fast. Any interaction should take
between 100ms and 200ms. Anything below 200ms will seem instant, but there's no
need to optimize below 100ms that as the human brain won't be able to perceive
the difference.

They are starting actions as soon as a key or click is pressed, not even waiting
for it to be released, which allow them to gain a few milliseconds here and
there, that build up to this overall speed feeling.

Another project they talked about is the Mozilla Developer Network (MDN). Theyr
recently joined forces with Microsoft, Chrome and the W3C to power the official
documentation of open web technologies. MDN is 6 million users per month,
translated in several languages. They are gradually moving from a free-form wiki
format to a more structured set of [JSON compatibility
data][1].

## Tools the docs room

The other room I attended was the "Tools the Doc", a kind of smaller "Write the
Docs" event, on only half a day.

### Branding styleguide

Zalando presented their syleguide and how they maintain it. Zalando is 14k
employees and more than 2k in tech. They created a styleguide to create
a consistent look and feel to all their products (both internal and external).
Building prototypes used to take weeks, it now take them days because of this
tool.

The styleguide is more than documentation, it is a set of CSS classes and React
components to build interfaces. Both front-end engineers and UX designers are
working on it, collaborating through GitHub. Each new addition can be discussed,
reviewed and tested before being added to the libraries.

The public face of the styleguide is available [here][2]. This front-end is an
application automatically generated from the inline documentation extracted from
the React components and CSS classes. This allow them to have the documentation
live at the same place as the code (using [documentationjs][3]. They can write
documentation in comments directly as markdown. Both React and CSS projects have
linters enabled, that are automatically tested during PR, to make sure the code
is following the agreed upon conventions.

### Finding a home for your docs

Another talk from Netlify was about where to host/store the doc website. Should
it be in the code? In another repo?

Netlify answer to that (after much trial and error) is to have documentation as
close to the code as possible (using JSDoc or other inline documentation
directly in the code). For more high-level documentation pages ("getting
started", etc), a `./docs` folder can hold several markdown files with more
details. This allow documentation to be browsed offline and as they are in the
same repo, they are more likely to be kept in sync.

Another repo will hold the actual documentation website, using any kind of
static website generator (Gatsby, Jekyll, Hugo, etc). This repo will fetch the
content from the code repo, build the website around it
and deploy it. They are using markdown-magic to enhance default markdown
capabilities (fetching external sourcing, adding basic templating, creating
a TOC, etc).

### Testing your API docs

Honza Javorek, from Apiary presented his various styles of development. From
Test Driven Development (TDD), to Behavior Driven Development (BDD) he ended up
doing RDD (Readme Driven Development). He starts by writing the README of the
project, with code examples of how the library is supposed to be working. It
forces him to think, as a user, what would be the best API to expose. Using
[doctest][4] he could even run the actual code
examples from the README and check that they were working. This had the other
advantage of making sure his documentation was up to date because he actually
wrote the documentation before writing the library and his documentation was
actually testable against the code.

He went even further and told us about the [API blueprint][5] format that let
you document your API endpoints in a way that is both human-readable and
machine-testable (through [Dredd][6]).

## Other rooms

I had the chance to see a few other talks on various other subjects throughout
the days.

### Package management bazaar and cathedral

An informative talk about the differences between distribution package managers
(like `apt`) and language-based package managers (like `pip` or `npm`). They
have very different approaches and don't work at all the same way.

Language package managers are used by builders to get access to a large number
of "lego bricks" they can use to build their own library/framwork on top of.
All packages are using the same language, and each package can be released
independently. The work put into each package is mostly a work of writing code.
It's a bazaar where everyone can add their own part.

Distribution package managers are used by users that want to have a stable set
of features that work well together. Packages downloaded can be of very
different languages and they need to all work together. The work done in each
package is mostly a work of coordination and glueing things together. It's
a cathedral carefully orchestrated by a few people.

### ElasticSearch

ElasticSearch had a 50mn talk about the evolution of the software from the early
days to the upcoming new release. The speaker was very good and did a nice blend
of live demos of new features with explanation of why those features where
needed. Lots of tooling to make sharding/replicating stuff easier (and with
better warning of what could go wrong and how to avoid it).

### CrateDB

[CrateDB][7] is an SQL database built on top of ElasticSearch.
Or, in their own words:

> A scalable SQL database optimized for search without any of the NoSQL
> bullshit"

SQL is fine, but search in it does not scale. ElasticSearch searches well, but
has no easy SQL syntax. So they build a mix of both, that is fully compatible
with any PostgreSQL client and also have a REST interface. They do not support
transactions, though. They use a mix of Antlr, Netty, Lucene and ElasticSearch,
allowing `SELECT` queries to target ES under the hood.

### Status pages with GitHub pages

The [Dariah research center][8] is using a Jekyll website
hosted on GitHub pages to host their status pages. As they have a large infra
with many moving parts, things break often, and they want to have a page
explaining quickly what is broken.

They didn't want to have this page on the same infra as the rest (to avoid
having the status page down with everything else). So they configured a Jekyll
website, using `yaml` files to represent each part of the infra. When something
is down, someone can just update the file and push back to GitHub. The website
will be automatically regenerated and hosted, with the list of up and down
services. Once the issue is resolved, a new commit/push and everything is back
to normal.

This setup does not even required them to learn new tools as `git` is common
tooling for everyone. It also had the added benefit of giving them history of
downtimes through the git history.

[1]: https://github.com/mdn/browser-compat-data
[2]: https://fabric-design.github.io/styleguide/#/general/4-typography
[3]: http://documentation.js.org/
[4]: https://pymotw.com/2/doctest/
[5]: https://apiblueprint.org/
[6]: http://dredd.readthedocs.io/en/latest/
[7]: https://crate.io/
[8]: https://dariah-de.github.io/status/
