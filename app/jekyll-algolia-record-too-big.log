{
  "id": "tag_list",
  "tag": "webperf",
  "posts": [
    "<p>Today in Lyon, France, was the We Love Speed conference. Its focus is on\neverything related to web performance. Even if the conference talks were only in\nFrench, I&#39;ll do this recap in English, to let more people learn from it. I took\na lot of notes while attending the conferences, directly in markdown format, and\nnow I&#39;m editing them, during my 4h30 train ride back home. I&#39;m not even going to\ntry to to a high level presentation of the state of webperf today; instead I&#39;ll\nfocus on writing short and concise recaps of each talk, with an overall\nconclusion at the end.</p>\n\n<h2>How to optimize 40k sites at once</h2>\n\n<p>This was a presentation by PagesJaunes, the french version of YellowPages. Their\nbrand used to be a big thing; before the Internet ever existed. Those yellow\npages were the only way to find a professional service in your area of living.\nNow, they&#39;ve totally embraced the web and have created a spin-off organization\ncalled Solocal.</p>\n\n<p>Solocal is a web agency that specialized in helping the online presence of SMBs\nby offering a package containing the development of a dedicated website, SEO,\nsocial media and ad presence as well as some advanced premium features (like\na store locator) on demand. Most of their customers have less than 10 employees,\nare not tech savvy and don&#39;t really know how to use a website anyway, but they\nknow that without one, they won&#39;t get customers in today&#39;s world.</p>\n\n<p>Most websites created by Solocal follow some dedicated templates (custom design\nin a premium feature). And because webperf has an impact on SEO, they had to\nimprove the perf of their templates to increase the SEO. Every change they made\nhad a direct impact on thousands of websites using this template.</p>\n\n<p>First talk of the day, and nice (albeit a bit long) introduction as to why\nwebperf are important. This really was wetting my appetite to know more about\nwhat they do. Unfortunately, the next part of the talk was supposed to be done\nby the CTO, which couldn&#39;t make it to the conference and recorded a video\ninstead. This was a last minute change to the program, and the conference team\ndidn&#39;t had time to properly setup the sound, so it was really hard to understand\nwhat he was saying. Anytime someone moved, the floor was creaking louder than\nthe video sound. I had to leave the room after 10mn of trying to understand the\ncontent. I figured my time would be better spent elsewhere, so I went downstairs\ndiscussing with a few people I met. I hope the final recording will allow us to\nknow more about the tech impact.</p>\n\n<h2>How to create a webperf culture in both dev and product</h2>\n\n<p>The second talk was much better; it&#39;s ranked my second favorite of the day. It\nwas presented by two people from leboncoin (the french equivalent of Craig&#39;s\nList), one from the product team and one from the dev team. </p>\n\n<p>Leboncoin is a pretty large company now, about 1400 employees; 400 of them in\nthe tech team. It grew significantly in the past years, the tech team almost\ndoubling in two years. Today, they have about 50 feature teams, handling about\n30M unique visitors per month. Scaling the tech team and keeping that many\npeople organised and synchronized is actually one of their main challenges\ntoday.</p>\n\n<p>But back to webperfs. Leboncoin actually started investing a lot in it because\nof a large perf regression in production they had in 2020. Their homepage was 7s\nslower than it used to be. They didn&#39;t caught it initially (they had no perf\nmonitoring), it&#39;s because their own customers and partners started complaining\nthat they realized something was not working properly. And when they saw that it\nhad a direct impact on their revenue, they tackled the issue by setting up\na Taskforce to remediate the regression.</p>\n\n<p>The taskforce was made of experts from their various domains (search, ad,\nauthentication, product details, etc). They also requested help from Google and\nJean-Pierre Vincent (a webperf consultant, also a speaker at We Love Speed).\nThey extracted a list of 40 things they should work on fixing. As they couldn&#39;t\nfix them all, they knew they had to prioritise them, but where not sure how to\ndo so.</p>\n\n<p>So they started identifying who their median user is, so they could optimize for\nthe median user. Turns out their median user is using a Galaxy S7 on a poor\nconnection with high latency. This was a defining information for them; they\nknew they had to optimize the mobile display (for a phone that was already\n5 years old) on a slow network.</p>\n\n<p>Leboncoin&#39;s motto is all about &quot;giving power to people to better live their day\nto day lives&quot;, by buying second hand stuff. So they couldn&#39;t really tell their\nusers to &quot;get a better phone&quot;. They had to make their website work for slow low\nend devices. So they took the most important item in their list, deployed\na fixed for it, analyzed the performance. Then they went to the second item,\ndeployed a fix for it, and analyzed again. And they went down their list like\nthis until the initial 7s regression was fixed. They even went a bit further.</p>\n\n<p>But they realized that it was a one-shot fix. If they didn&#39;t invest in long term\nperformance tracking and fixing, they will have to do it all over again in\n6 months. Performance optimization is not a sprint, it&#39;s a marathon and you have\nto continually monitor it. Which is what the did. They started by adding some\nlive performance monitoring, logging the results in Datadog and sending a Slack\nalert in relevant channels when one metric was above a defined threshold. It did\nnot prevent pushing slow code to production, but at least they had the history\nand alerts when things went wrong. They monitored only the most important pages\n(homepage, search results and details), and measured on different devices.</p>\n\n<p>The second step was to be able to catch performance regression before they hit\nproduction. They added a check on the bundle size of their JavaScript. This\nmetric is pretty easy to get, and they pluggued this to their GitHub issues, so\nwhenever the bundlesize difference is too large (&gt; 10%) between the PR and the\ncurrent code; the PR cannot be merged. Again, they tracked the change overtime\nto have the history. </p>\n\n<p>They also added automated Lighthouse tests in their CI. Lighthouse is not\na perfect tool, and its score shouldn&#39;t be taken as an absolute truth. Depending\non your stack and use case, some metrics are more important than others. Still,\nit&#39;s an invaluable tool to make sure everybody in the team can talk about the\nsame thing. Without this data, it would just have been another opinion. They\nadded thresholds on some of those metrics in the same vein as the bundlesize\nlimit: it it goes too far above a threshold; the PR is blocked. This forced\ndeveloper, designers and product owners to discuss the decisions, with an\nobjective metric.</p>\n\n<p>The next step was to teach people internally about all those metrics. What they\nmean and why they are important. They created a set of  slides to explain each\nmetric, to each internal audience. For example, they had one talk to explain why\nthe LCP (Largest Contentful Paint) is important to a designer, and how to keep\nit low. But also the same talk to explain it to a product manager, or developer,\nwith a specific explanation and examples so everybody knows why it&#39;s important,\neven they don&#39;t care about them for the same reasons. That way, the whole teams\nhad a shared goal and not opposite objectives.</p>\n\n<p>And the last step is to always keep one step ahead. The webperf field evolves\nmore and more rapidly; there are new elements to learn every few months.\nBrowsers ships new feature that could help or hinder webperf, and people need to\nbe kept up to date with them. </p>\n\n<p>Overall, this whole production issue turned into a taskforce that finally turned\ninto a whole company-wide shift. Webperf talks are common, both by developers,\ndesigners and product people. Their team keeps up to date with the latest news\nin the webperf world, they closely follow what Google or Vercel is doing, and\nthose metrics became KPIs that everybody can understand.</p>\n\n<p>Still, even with all those progress they know that they are not perfect, and\nthey have to optimize for some metrics instead of others. When making one score\nbetter, they might make another worse. They&#39;re aware of that, but because they\nhave defined which metrics are more important than others, they can usually\ndefine if the tradeoff is acceptable.</p>\n\n<h2>Why you need a markup expert</h2>\n\n<p>Jean-Pierre Vincent then presented one of the most technical talks of the day.\nJumping right into it, he picked the example of a homepage with a hero image,\nsome text and a CTA and showed how, in 2021, this could be optimized. The goal\nis to make sure it delivers its message as fast as possible, even on a low end\nmobile device with a slow connection.</p>\n\n<p>His talk is pretty hard to recap, because of the amount of (French-only) jokes\nin it, and the way it was swinging from high-level meta considerations into deep\nbrowser-specific hacks. The crux is that JavaScript never solves webperf issues.\nIt only create them. Sure it comes with solutions to cancel the issues it\ncreates, making it neutral, but it will never make your pages load faster, no\nmatter how optimized it is. If you really want to gain in performance, you have\nto invest in the underlying fundamental standards: HTML and CSS.</p>\n\n<p>We should strive to develop every single component with a &quot;good enough&quot; version\nthat works without JavaScript. Instead of having either nothing or an empty grey\nblock while waiting for the JS to load, we could at least have a MVP without all\nthe bells and whistles, but that at least look like the full component, and act\npartly like one. He gave the example of a slideshow (carousel) component. With\nstandard HTML and CSS, it is possible to have something that already works\npretty well without a single byte of JavaScript. </p>\n\n<p>In real life, we barely have 1% of our users that navigate without JavaScript.\nA very few proportion manually disables JavaScript, the majority of those 1% are\npeople either behind a corporate proxy that wrongfully blocks scripts, or people\non a poor connection that don&#39;t <strong>yet</strong> have their JavaScript downloaded. If\nyou&#39;re a small startup, there is no incentive in optimizing for 1% of your\nusers. But if you&#39;re a large company, 1% of your users can be hundred of\nthousands of dollars of revenue. In any case, forcing yourself to build for\nthose 1% without JavaScript will only make you create faster components.</p>\n\n<p>It doesn&#39;t mean you need to build for the smaller common denominator and serve\nthis pure HTML/CSS version to all your users. But it should at least be the\nversion they can interact with <strong>while your JavaScript is loading</strong>. Take for\nexample a datepicker. Datepickers are incredibly useful components that many\nsite needs. But the amount of JavaScript required for it to work properly\n(handling date formatting in itself is a complex topic) is often quite large.\nWhat about using the default, standard, HTML datepicker provided by the browser.\nAnd load the full fledged datepicker only when the user will need it (for\nexample on focus of the actual field). That way the intial page load is fast,\nand the datepicker is required only when it is needed.</p>\n\n<p>Jean-Pierre then moved onto explaining the best ways to load an image if we want\nit to be displayed quickly. An image is worth a thousand words, as the saying\ngoes, and it is even truer on homepage where nobody will read your text. So you\nneed to have your main image displayed as fast as possible. He warned us about\nnot using background-image in CSS for that (even if it comes with the nifty\nbackground cover properties) because some browser will put background images at\nthe bottom of their download priority list, prefering regular <img> tags\ninstead. Modern browsers now have object-fit that is similar to backround-cover\nbut for real image tags. For older browsers, you can hack your way around by\nadding a fake <img> tag referencing the same image as the one in\nbackground-image and adding a display:none.</p>\n\n<p>On chrome the problem is even more complex as the image download priority is\ncalculated based on the image visibility. If it is not in the viewport, it will\nbe put at the bottom of the priority list. This seems pretty clever, but the\ndrawback is that the browsers needs to know the image placement in the page\nbefore downloading them, so it needs to download the CSS before it downloads the\nimages. The suggested way around this limitation if you really need to download\none image as fast as possible is to add a <link rel=\"preload\"> tag for this\nimage. Preloading is a very interesting concept, but once again we have to be\ncareful not to use it for everything. If we mark all our images for preloading\nit&#39;s like we&#39;re not preloading anything.</p>\n\n<p>Once we know how to download an image as soon as possible, we have to make sure\nto download the smallest viable image. The srcset attribute allow us to define\n(in addition to the default src attribute) specific images to load based on the\ncurrent image display size. The syntax is very verbose and can quickly turn\npretty complex to maintain as picking the right image depends on three factors:\nthe current screen resolution, the Device Pixel Ratio (retina or not) and the\nrelative size of the image compared to the page. The last two are tricky because\nscreens are getting higher and higher pixel ratio (3 or more) and the relative\nsize of the image is linked to your RWD breakpoints. This creates a larger and\nlarger number of combination, making this whole syntax harder and harder to\nwrite.</p>\n\n<p>Still, has this is a standard syntax, directly at the HTML level, it will work\non every browser (eventually) and will be much better than any JavaScript-based\nsolution (or even CSS-based solution for that matter).</p>\n\n<p>The last advice he gave us on images was to make sure we are not uselessly\ndownloading an image that is not going to be display (because we don&#39;t display\nthem on mobile for example). As usual, the fastest way to transfer bytes on the\nwire is to not transfer them at all, so if an asset is not going to be used, it\nshould not be sent. But if you have a lot of images to be displayed, you need to\nensure you&#39;re giving them width and height dimensions, so at least their\nrespective space in the layout is reserved and the page does not jump as images\nare downloaded.</p>\n\n<p>Speaking of lazy loading images, there is no clear answer if we should be using\ngray placeholders, blurry version of the images, a spinner or a brand logo while\nwaiting for an image to load. There is no one size fits all, it all depends on\nthe use case, the specific page and the other images around. This question needs\nto be answered by the design team, not the devs.</p>\n\n<p>There was a lot of content packed into this talk, I would highly suggest you\nhave a look at the recording and the slides (or even book a private consultant\ngig with him) because I can&#39;t make it justice. Still, the last topic he\naddressed was the font loading. The best way to load fonts being to define\na series of fallbacks, from the best to the worst. The best being the font being\nalready installed locally on the user computer, the worst being an old TTF/OTF\nformat to be downloaded online. Then there is the question of font swapping: if\nthe fonts needs to be downloaded, you should at least present the text in\na fallback font while the font is loading. If your default font and real font\nare really similar, the swap should be almost imperceptible. If they are very\ndifferent, the swap could create a noticeable jump (because the real font has\nlarger/smaller letters it could make buttons appear on two lines after the swap\nfor example). In that case, the suggest trick is to scale the default font\nup/down so it takes roughly the same size as the final font. That way the swap\nwill seem less brutal.</p>\n\n<p>All those examples were highly interesting, but they will also most probably be\noutdated in one year or two. The main important thing to remember here is that\nwe need to invest into markup specialists, people that know the underlying HTML\nand CSS properties, keep up to date with the way they evolve and are integrated\nby browsers. Knowing all those properties and keeping up to date is a full time\njob, and you can&#39;t expect a front-end engineer to be able to juggle all that\ninformation while also keeping up to date with the JavaScript ecosystem (that is\nevolving at least as fast). It&#39;s time we better recognize markup specialists as\nexpert, and what they bring to the webperf front.</p>\n\n<h2>Micro-frontends and their impact on webperf at Leroy-Merlin</h2>\n\n<p>This one was the most impressive talk of the day. How Leroy-Merlin (5th french\ne-commerce website) rewrote their whole front into a micro-frontend architecture\nand what the impact on webperf was.</p>\n\n<p>For a bit of context, Leroy-Merlin has 150 physical stores, they do a mix of\nonline and physical business while most of their competitors are pure players\n(like ManoMano, which was actually doing a talk in the same room right after\nthis one). But back to Leroy-Merlin: their traffic is mostly (55%) coming from\nmobile, and the average user journey is 7 pages long. This is going to become\nimportant data for the rest of the talk.</p>\n\n<p>The two speakers were tech leads of the front. They were upfront about the KPIs\nthey wanted to optimize: great SEO, quick Time to Market (ability to release new\nfeatures quickly), fast performances, data freshness and resiliency. They\nquality/price/availability of the products in store isn&#39;t part of their scope.\nThey need to make sure the website loads fast and displays relevant information\nno matter the conditions.</p>\n\n<p>Before their rewrite, they used to have one large monolith and a dev team of\na bit more than 100 devs. This created a lot of frictions in their deployments\nas everybody had to wait in a queue for releasing their part of the code. Their\nwebperf was good, but they had to manually deploy their servers and had some\nissues with their load balancer (sticky sessions that dropped customers when\na server was down).</p>\n\n<p>Individually those problems weren&#39;t too bad. But all together, it meant it was\ntime to restart from scratch and think of a solution that would fix all those\nproblems at once: automated deployments, stateless machines and autonomous\nteams. For the infra part they embraced the Infrastructure as Code with Docker,\nand for the front went with a micro front-end architecture, where each page is\nsplit into &quot;fragments&quot;. They have one fragment for the navigation bar, one for\nthe &quot;add to cart&quot; button, one for the similar items, one for calculating the\nnumber of items in stock, etc. Each fragment is owned by a different team (made\nup of front/back engineers, product owner, manager and designer). </p>\n\n<p>Each team can then pick the best stack for their specific job. The most complex\ncomponents are made in React (about 5% of them), while the vast majority are\nmade of Vanilla JavaScript. Because they split a large page into smaller,\nsimpler, components they didn&#39;t need a heavy framework because each fragment was\ndoing one simple thing. This allowed them to heavily simplify the complexity of\ntheir code, leading to a much better Time to Market. Each fragment being like\na self-contained component, along with assets and specific logic, it&#39;s also\neasier to remove dead code than when it&#39;s sprawled over the whole codebase.</p>\n\n<p>They have a backend UI tool that let them build custom pages by drag&#39;n&#39;dropping\nfragments (which is also securely saved as YAML configuration files, so they can\nredeploy with confidence). The final page is then assembled in the backend when\nrequested. It picks the page template (homepage, listing, or product detail),\nand replaces the 30 or so fragment placeholders with the corresponding fragment\ncode. This fully assembled page is then send to the browser and kept in cache\nfor future request. Thus, the backend job is also heavily simplified. It mostly\ndoes templating work, once again reducing the complexity.</p>\n\n<p>One limitation of such an architecture is that any personalization data (current\nuser, number of items in cart, availability of a product) cannot be served\ndirectly by the backend, and has to be fetched by the frontend. But because 99%\nof the page has already been pre-rendered on the server, fetching those data\nrequires only a minimal amount of JS and is quickly executed in the front-end.\nBecause their average user journey is 7 pages long, they decided that it wasn&#39;t\nworth downloading a full JavaScript framework for only 7 pages and so they try\nto really do most of the stuff in vanilla JavaScript.</p>\n\n<p>But, this choices creates another limitation. Because each fragment is isolated,\nit means that code is often duplicated. And because no framework is used, it\nmeans that all the fancy tooling and helpers that improves the Developer\nExperience are missing. Also, coding without a framework proved to make hiring\nharder. For all those reasons, they extracted some of the most common shared\ncomponents into their own private modules (like the design system, the API\nconnection layer, polyfills, etc) into their own private npm module that each\nfragment can import. For isolating CSS rules, the prefix each CSS selector with\nthe unique ID of the matching fragment.</p>\n\n<p>Having the full page being split into smaller chunks also allowed them to\nincrease their resilience. They could define which fragments are considered\nprimary or secondary. A primary fragment is needed for the page to work (like\ndisplay the product, or the &quot;add to cart&quot; button). If this fragment fails to\nbuild, for whatever reason, then the page needs to fail loading. On the other\nhand, secondary fragments (like a &quot;similar item&quot; carousel, or the page footer)\nare considered secondary and if they fail loading, they are simply ignored and\nremoved from the markup. This allowed them to be more resilient to errors, and\nbetter scale in case of high traffic spikes. They went even further and made the\nsecondary fragment lazyload: their JavaScript is loaded only when the fragment\nis about to enter the viewport, making the first page load really fast.</p>\n\n<p>But that&#39;s not all, and they went even further with their caching mechanism. As\nwe&#39;ve seen above, they cache the backend response of the build pages. But what\nif the page layout changes? What if a product is no longer in stock and the\nlayout seems to be completely changed? They couldn&#39;t use revved urls because\nthey wanted to keep a good SEO and unique URLs. They also didn&#39;t want to\nintroduce a TTL because it would have drastically improved the complexity of\nhandling the cache.</p>\n\n<p>Instead, they opted for a reactive approach with a low TTL. Every page is cached\nin the browser for a short amount of time (I don&#39;t remember if they said the\nexact value, but I expect 1 or 2 seconds). This is low enough so a regular user\nwon&#39;t notice, but high enough that thousands of users on Black Friday pressing\nF5 won&#39;t kill the server. But the same page is cached in the server forever. The\nvery clever and tricky part is that they update their server cache whenever\ntheir database is updated. The listen to any change in their config database,\nand if a change requires a cached page to be regenerated, they regenerate it\nasynchronously. That way users still have fresh data, but the server isn&#39;t under\na lot of pressure.</p>\n\n<p>In addition to all that, they even have different pages generated based on the\nUser-Agent. An modern browser won&#39;t have all the polyfills added, while an old\none might have. Some goes for mobiles that might not require some part of the\nmarkup/assets, so they are skipped during the page creation, once again for\nfaster load.</p>\n\n<p>I told you it was the most impressive talk of the day. They went very far into\nthe micro-frontend direction, and even beyond, taking full advantage of what its\nmodularization approach made possible. This full rewrite required\nsynchronization of the data, front, back and infra teams and also a full\nreorganization of the feature teams. This went far beyond a tech project, and\nhad impact on the whole company organization.</p>\n\n<h2>SpartacUX, ManoMano&#39;s rewrite to micro-frontend</h2>\n\n<p>The next conference was pretty similar to the one presented by Leroy-Merlin.\nThis time it was ManoMano, actually one of their competitor, explaining\na similar approach the had. Both talks being one after the other, we couldn&#39;t\nhelp but compare to what we just saw in the talk behind. ManoMano&#39;s\ninfrastructure is pretty impressive as well, but Leroy-Merlin went so far ahead\nit was hard to be as excited about this second talk as I was for the first one.\nThere was also a lot of overlap with what Le Bon Coin presented earlier in the\nmorning about how they track webperf stats in their PR and dashboards.</p>\n\n<p>ManoMano started as a Symfony backend with Vanilla JavaScript. They had trouble\nrecruiting Vanilla JS developers, so they moved the front to React. This hurt\ntheir SEO as their SSR wasn&#39;t properly working with React. They also still had\nthe previous monolith as the backend, and felt like they were duplicating code\non both ends, that their performance was getting even worse, and people in the\nteam were struggling with the new complexity to orchestrate.</p>\n\n<p>So they started the really cleverly named SPArtacUX project. A way to bridge the\nSingle Page Application with a better User Experience. The goal was to have\na simple codebase for the dev team, while transferring as few bytes as possible,\nfor faster rendering. They opted for micro-frontend architecture (I see a trend\nhere), using Next.js (I see a trend here as well) because it offered nice SSR\nand they were already proficient with React. They moved to TypeScript for type\nrobustness and used Sass for CSS. As a side note, I still don&#39;t really\nunderstand why so many companies keep using Sass for their CSS stack (it&#39;s slow,\nit leaks styles, it&#39;s non-standard; Tailwind would be a better choice IMO,\nespecially when you already have a design system).</p>\n\n<p>They also started measuring Web Vitals and bundle size in all their production\nreleases and Pull Requests. They pluggued Lighthouse, WebPageTest, Webpack\nBundle Analyzer and Chrome Dev Tools to their CI to feed Datadog dashboard and\nstatic reports. When they had enough data to see a trend, they started to\noptimize. Their first target were the third part tracking scripts that were\nheavily slowing the page down. Those tags are very hard to remove because they\ncan have a business impact; you cannot remove too much data otherwise you&#39;re\nblind to how your business is performing. They had to get an exhaustive list of\neverything that was loaded and remove the ones that were no longer used.</p>\n\n<p>Then they had to rewrite a fair number of their components that they thought\nwere responsive, but were actually downloading both a desktop and mobile version\nand hiding one of the two based on the current devices. This made a lot of\nHTML/CSS and even sometimes images to download for not even displaying it. They\nput a CDN in front of all their pages. Just like Leroy-Merlin, they build the\npages based on a layout and placeholders to replace with fragments. </p>\n\n<p>They pay special attention at optimizing the loading order of assets, only\nloading assets that are in the current viewport, lazy loading anything else.\nThey invested a lot of time into code splitting and tree shaking to only load\nwhat they really needed in their final build. They also made sure any inline SVG\nicon asset was only includes once, and the other icons were referencing the\nfirst one, avoiding downloadin several times the same heavy SVG icon.</p>\n\n<p>In conclusion, they did a really good job on their rewrite, a bit like a mix of\nLeroy-Merlin on their micro-frontend split and Le Bon Coin on their webperf\nautomation monitoring; but it felt like I had already seen that today. I&#39;m sure\nif I would have seen this talk first, I would have been more ecstatic about it.</p>\n\n<h2>What is faster than a SPA? No SPA.</h2>\n\n<p>The last talk of the day was by Anthony Ricaud, which made a clean and concise\ndebunking of the myth that SPA are inherently faster because they need to only\nload the diff that changes between two pages. Because he was going against what\nis a commonly accepted idea, he had to put up in the right mindset first by\nreminding us of cognitive bias and rhetorical techniques we&#39;re all guilty of.</p>\n\n<p>Then he showed, with many example recording (of actual websites we had seen\nduring the day), how a version without SPA (so, with simpler GET requests to\na server) was actually faster. The reasoning is pretty simple, and went with\nwhat Jean-Pierre Vincent said earlier: JavaScript will never make your pages\nfaster; at best it will offset its slowness.</p>\n\n<p>The main reasons for that are that with a SPA, you need to download a lot of\nblocking JavaScript which you don&#39;t have to with classical HTTP navigation. Also\nwith a SPA, you need to get a JSON representation of your state, transform it\ninto a VDOM, then update the existing DOM. With classical HTTP navigation you\ncan start rendering the DOM on the fly, while you&#39;re actually still downloading\nit though HTTP.</p>\n\n<p>In addition, when doing classical HTTP navigation, your browser UI will let you\nknow if the page is loading, while with a SPA it&#39;s up to the SPA to have its own\nloading indicators (which they usually don&#39;t have, or trigger too late). This\ntied well with what Leroy-Merlin was saying earlier in that for 95% of their\nfragments, they use pure Vanilla JS, and with Jean-Pierre Vincent once again in\nthat you can already do a lot with pure standard HTML/CSS that JavaScript will\nonly be needed for progressive enhancement.</p>\n\n<p>He then went on doing a demo of HOTWire (HTML Over The Wire), which is an hybrid\nway that should take the best of both worlds. It would use a limited amount of\nJavaScript, plugging itself on standard HTML markup, to only refresh part of\na page in an obstrusive manner. The idea is to tag parts of our HTML pages with\ntags indicating that an area should be updated without the whole page being\nrefreshed. The minimal JavaScript framework would then query asynchronously the\nnew page; the server would return an HTML version of the new page, filter only\nthe area it needs to update and swap the old area with the new one in the\ncurrent page.</p>\n\n<p>To be honest, the idea seems interesting, but the syntax seemed to be a bit too\nverbose and still a bit uncommon. Made me think of Alpine.js which follows\na similar pattern of annotating HTML markup with custom attributes, to\nstreamline JavaScript interaction with it. I&#39;m still unsure if this is a good\nidea or not; it reminds me of Angular going fully in that direction and it\ndidn&#39;t really went well for them, it created an intermediate layer of &quot;almost\nHTML&quot;.</p>\n\n<h2>Conclusion</h2>\n\n<p>I&#39;m really glad I could attend physically this event. It has been too long since\nI could go to conferences because of the COVID situation. Having a full day of\nwebperf peeps sharing their discoveries, and seeing how far the webperf field\nwent in the past years has been really exciting. It&#39;s no longer a field only for\ndeep tech people passionate about shaving off a few ms here and there, it has\nnow a proven direct impact on SEO, revenue, trust and team organization.</p>\n\n<p>Thanks again to all the organizers, speakers and sponsors for making such an\nevent possible!</p>\n",
    "<p>Quand on cherche à optimiser les performances de son site web, il y a trois\néléments essentiels à faire avant toute chose. Trois méthodes très\nsimples à mettre en place et qui apportent un retour direct et flagrant sur la\nvitesse de chargement.</p>\n\n<p>Ces trois méthodes sont la concaténation, la compression et le cache. J&#39;ai déjà\nabordé celles-ci lors d&#39;une <a href=\"https://www.youtube.com/watch?v=ecc1zudWmX4\">présentation aux\nHumanTalks</a> de Septembre 2014,\nmais nous allons les détailler dans la suite de cet article.</p>\n\n<h2>Concaténation</h2>\n\n<p>Le principe de la concaténation est de regrouper plusieurs fichiers de même\ntype en un seul, afin de se retrouver avec moins de fichiers finaux\nà télécharger. Les fichiers qui profitent le plus de ce système sont les\nfichiers CSS et Javascript.</p>\n\n<p>La nature même du téléchargement d&#39;assets fait que notre navigateur doit payer\ncertains coûts, en millisecondes, à chaque nouvel élément téléchargé. Ces coûts\nsont de diverses natures:</p>\n\n<h3>TCP Slow start</h3>\n\n<p>TCP, le protocole de connexion qu&#39;utilise HTTP, possède un mécanisme de\nslow-start qui lui permet de calculer la vitesse optimale de transmission de\nl&#39;information. Pour parvenir à ce résultat, il doit effectuer plusieurs\naller-retours entre le client et le serveur, en envoyant de plus en plus en plus\nde données, pour calculer la vitesse maximale possible d&#39;émission/réception. </p>\n\n<p>Si on envoie une multitude de petits fichiers, la transmission n&#39;a jamais le\ntemps d&#39;atteindre sa vitesse optimale et doit recommencer ses aller-retours\npour le prochain fichier. En groupant les fichiers en un fichier de plus grande\ntaille, le coût de calcul n&#39;est payé qu&#39;une seule fois et le reste du fichier\npeut se télécharger à la vitesse maximum.</p>\n\n<p>À noter que maintenir les connexions à votre serveur en <code>Keep-Alive</code> permet de\nréutiliser une connexion d&#39;un asset vers le suivant et donc de ne payer le coût\nde calcul qu&#39;une fois. Malheureusement, activer le <code>Keep-Alive</code> sur un serveur\nApache risque aussi de limiter le nombre de connexions parallèle que votre\nserveur peut maintenir.</p>\n\n<h3>SSL</h3>\n\n<p>De la même manière, si votre serveur utilise une connexion sécurisée, il y a un\néchange de clés entre le client et le serveur qui s&#39;effectue pour vérifier que\nles deux sont bien qui ils annoncent être. Ici encore, le coût de cet échange\nest payé sur chaque asset téléchargé. Mettre les fichiers en commun permet donc\nde ne payer le coût de cet échange qu&#39;une seule fois.</p>\n\n<h3>Connexions parallèles</h3>\n\n<p>Finalement, il y a une dernière limite, purement du coté du navigateur cette\nfois-ci : le nombre de connexions parallèles. La norme HTTP indique qu&#39;un\nnavigateur devrait ouvrir un maximum de 2 connexions parallèles vers un même\nserveur. Techniquement, les navigateurs récents ont augmenté cette limite à une\nvaleur entre 8 et 12 car 2 était beaucoup trop restrictif.</p>\n\n<p>Cela signifie c&#39;est que si vous demandez à votre page web de télécharger\n5 feuilles de style, 5 scripts et 10 images, le navigateur ne va lancer le\ntéléchargement que des 12 premiers éléments. Il commencera le téléchargement du\n13e uniquement une fois qu&#39;un des 12 premiers sera arrivé, et ainsi de suite.\nIci encore, la concaténation vous permet de laisser plus de canaux disponibles\npour télécharger les autres assets de votre page.</p>\n\n<p>Les fichiers CSS et Javascript se concatènent très bien. Il suffit simplement\nde créer un fichier final qui contient le contenu mis bout-à-bout de tous les\nfichiers initiaux. Votre processus de build devrait pouvoir s&#39;en charger sans\nproblème, mais un solution simple peut s&#39;écrire en quelques lignes :</p>\n<div class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"nb\">cat</span> ./src/<span class=\"k\">*</span>.css <span class=\"o\">&gt;</span> ./dist/styles.css\n<span class=\"nb\">cat</span> ./js/<span class=\"k\">*</span>.js <span class=\"o\">&gt;</span> ./dist/scripts.js\n</code></pre></div>\n<p>À noter que la concaténation d&#39;images (CSS Sprites) est aussi possible, mais\nnous ne l&#39;aborderons pas dans cet article.</p>\n\n<h2>Compression</h2>\n\n<p>Maintenant que nous avons réduit le nombre de fichiers, notre deuxième tâche\nva être de rendre ces fichiers plus légers, afin qu&#39;ils se téléchargent plus\nrapidement.</p>\n\n<p>Pour cela, il existe une formule magique formidable nommée Gzip qui permet de\nréduire de 66% en moyenne le poids des assets textuels.</p>\n\n<p>La bonne nouvelle c&#39;est que la majorité des assets que nous utilisons dans la\ncréation d&#39;un site web sont du texte. Les briques principales comme le HTML, le\nCSS et le Javascript bien sur, mais aussi les formats classiques de retour de\nvotre API : XML et JSON. Et beaucoup d&#39;autres formats qui ne sont en fait que\ndu XML déguisé : flux RSS, webfonts, SVG.</p>\n\n<p>Gzip, et c&#39;est assez rare pour le souligner, est parfaitement interprété par\ntous les serveurs et tous les navigateurs du marché (jusque IE5.5, c&#39;est dire).\nIl n&#39;y a donc aucune raison de ne pas l&#39;utiliser.</p>\n\n<p>Si un navigateur supporte le Gzip, il enverra un header <code>Accept-Encoding: gzip</code>\nau serveur. Si le serveur décèle ce header dans la requête, il compressera le\nfichier à la volée avant de le retourner au client, en y ajoutant le header\n<code>Content-Encoding: gzip</code>, et le client le décompressera à la reception.</p>\n\n<p>L&#39;avantage est donc d&#39;avoir un fichier de taille réduite qui transite sur le\nréseau, avec en contrepartie le serveur et le client qui s&#39;occupent\nrespectivement de la compression/décompression. Sur n&#39;importe quelle machine\nissue des 10 dernières années, l&#39;overhead de la compression/décompression en\ngzip est absolument négligeable. Par contre, le fait d&#39;avoir un fichier bien\nplus léger qui transite sur le réseau permet des gains très importants.</p>\n\n<p>Les librairies de compression Gzip sont disponibles sur tous les serveurs du\nmarché, il suffit généralement simplement de les activer en leur indiquant les\ntypes de fichiers qui doivent être compressées. Vous trouverez ci-dessous\nquelques exemples sur les serveurs les plus connus :</p>\n\n<h4>Apache</h4>\n<div class=\"highlight\"><pre><code class=\"language-apache\" data-lang=\"apache\"><span class=\"p\">&lt;</span><span class=\"nl\">IfModule</span><span class=\"sr\"> mod_deflate.c</span><span class=\"p\">&gt;\n</span>  <span class=\"p\">&lt;</span><span class=\"nl\">IfModule</span><span class=\"sr\"> mod_filter.c</span><span class=\"p\">&gt;\n</span>    <span class=\"nc\">AddOutputFilterByType</span> DEFLATE \"application/javascript\" \"application/json\" \\\n    <span class=\"err\">\"</span>text/css\" \"text/html\" \"text/xml\" [...]\n  <span class=\"p\">&lt;/</span><span class=\"nl\">IfModule</span><span class=\"p\">&gt;\n&lt;/</span><span class=\"nl\">IfModule</span><span class=\"p\">&gt;\n</span></code></pre></div>\n<h4>Lighttpd</h4>\n<div class=\"highlight\"><pre><code class=\"language-lighttpd\" data-lang=\"lighttpd\">server.modules += ( \"mod_compress\" )\ncompress.filetype  = (\"application/javascript\", \"application/json\", \\\n\"text/css\", \"text/html\", \"text/xml\", [...] )\n</code></pre></div>\n<h4>Nginx</h4>\n<div class=\"highlight\"><pre><code class=\"language-nginx\" data-lang=\"nginx\"><span class=\"k\">gzip</span> <span class=\"no\">on</span><span class=\"p\">;</span>\n<span class=\"k\">gzip_comp_level</span> <span class=\"mi\">6</span><span class=\"p\">;</span>\n<span class=\"k\">gzip_types</span> <span class=\"nc\">application/javascript</span> <span class=\"nc\">application/json</span> <span class=\"nc\">text/css</span> <span class=\"nc\">text/html</span> <span class=\"nc\">text/xml</span>\n<span class=\"s\">[...]</span><span class=\"p\">;</span> \n</code></pre></div>\n<p>S&#39;il y a bien une optimisation de performance qui nécessite peu de travail\nà mettre en place et qui améliore grandement les performances de chargement,\nc&#39;est bien le Gzip. Cela ne nécessite aucun changement sur les fichiers servis,\nuniquement une activation de config sur le serveur.</p>\n\n<h3>Minification</h3>\n\n<p>Pour aller plus loin, vous pouvez aussi investir sur la minification de vos\nassets. HTML, CSS et Javascript sont encore une fois les meilleurs candidats\npour la minification. </p>\n\n<p>La minification est un procédé qui va ré-écrire le code de vos assets dans une\nversion qui utilise moins de caractères, et qui donc pésera moins lourd sur le\nréseau. D&#39;une manière générale cela va surtout supprimer les commentaires et\nles sauts de ligne, mais des minificateurs plus spécialisés pourront renommer\nles variables de vos Javascript en des valeurs plus courtes, regrouper vos\nsélecteurs CSS ou supprimer les attributs redondants de vos pages HTML.</p>\n\n<p>L&#39;ajout d&#39;un processus de minification est plus complexe que l&#39;activation du\nGzip, et les gains sont aussi moins importants. C&#39;est pourquoi nous vous\nconseillons de toujours commencer par la compression Gzip.</p>\n\n<h2>Cache</h2>\n\n<p>À présent que nous avons réussi à limiter le nombre de fichiers et à faire\nbaisser leur poids, la prochaine étape est de les télécharger le moins souvent\npossible.</p>\n\n<p>L&#39;idée principale ici est qu&#39;il est inutile de faire télécharger à votre\nvisiteur un contenu qu&#39;il a déjà téléchargé et possède donc en local sur son\nposte.</p>\n\n<p>Nous allons commencer par expliquer comment fonctionne le cache HTTP car c&#39;est\nun domaine qui est généralement mal compris des développeurs. Il y a en fait\ndeux principes fondamentaux à comprendre dans le cache HTTP: la <em>fraicheur</em>, et\nla <em>validation</em>.</p>\n\n<h3>Fraicheur</h3>\n\n<p>On peut voir la fraicheur d&#39;un asset comme une date limite de consommation.\nLorsque l&#39;on télécharge un élément depuis le serveur, celui-ci nous l&#39;envoie\naccompagné d&#39;un header indiquant jusqu&#39;à quelle date cet élément est encore\nfrais.</p>\n\n<p>Si jamais le client à besoin à nouveau du même élément, il commence par\nvérifier la fraicheur de celui qu&#39;il a en cache. S&#39;il est encore frais, il ne\nfait pas de requête au serveur, et utilise directement celui qu&#39;il a sur son\ndisque. On ne peut pas faire plus rapide, car il n&#39;y a alors absolument aucune\nconnexion réseau impliquée.</p>\n\n<p>Par contre, si jamais la date de fraicheur est dépassée, alors le navigateur va\nlancer une nouvelle requête au serveur pour récupérer la nouvelle version.</p>\n\n<p>En HTTP 1.0, le serveur retourne un header <code>Expires</code> avec la date limite de\nfraicheur. Par exemple: <code>Expires: Thu, 04 May 2014 20:00:00 GMT</code>. Dans cet\nexemple, si jamais le navigateur demande à nouveau le même asset avant le 4 Mai\n2014 à 20h, alors il le lira depuis son cache, sinon il interrogera le serveur.</p>\n\n<p>Cette notation a un défaut majeur dans le fait que les dates sont fixées de\nmanière absolue. Cela signifie que le cache de tous les clients perdra sa\nfraicheur en même temps. Et vous aurez donc potentiellement tous les clients\nqui feront une nouvelle requête vers votre serveur en même temps pour se mettre\nà jour, ce qui peut générer un très fort pic de charge à cet instant.</p>\n\n<p>Pour limiter cela et donner plus de flexibilité dans la gestion de la\nfraicheur, en HTTP 1.1, un nouveau header à été introduit : <code>Cache-Control</code>.\nCelui-ci accepte plusieurs arguments qui permettent de gérer plus finement la\nmanière de mettre en cache, et celui qui nous intéresse ici est <code>max-age</code> qui\npermet de définir une durée relative de fraicheur, en secondes.</p>\n\n<p>Votre serveur peut donc répondre <code>Cache-Control: max-age=3600</code> pour indiquer\nque l&#39;asset est encore frais pendant 1h (3600 secondes). En faisant ainsi vous\npouvez espacer les appels sur une plus longue période.</p>\n\n<h3>Validation</h3>\n\n<p>La deuxième composante du cache est la <em>validation</em>. Imaginons que notre asset\nai terminé sa période de fraicheur, nous allons donc récupérer une nouvelle\nversion de celui-ci sur le serveur. Mais il est possible que l&#39;asset n&#39;ait pas\nréellement changé sur le serveur depuis la dernière fois. Il serait alors\ninutile de retélécharger quelque chose que nous avons déjà dans notre cache.</p>\n\n<p>Le principe de validation permet au serveur de gérer cela. Soit l&#39;asset du\nclient est identique à l&#39;asset du serveur, dans ce cas le client peut garder sa\nversion locale. Soit les deux sont différents et dans ce cas le client doit\nmettre à jour son cache avec la version distante.</p>\n\n<p>Lorsque le client a récupéré l&#39;asset pour la première fois, le serveur lui\na répondu avec un header <code>Last-Modified</code>, par exemple <code>Last-Modified: Mon, 04\nMay 2014 02:28:12 GMT</code>. La prochaine fois que le client fera une requête pour\nrécupérer cet asset, il renverra la date dans son header <code>If-Modified-Since</code>,\npar exemple <code>If-Modified-Since: Mon, 04 May 2014 02:28:12 GMT</code>.</p>\n\n<p>Le serveur compare alors la date envoyée et celle qu&#39;il possède de son coté.\nSi les deux correspondent, alors il renverra un <code>304 Not Modified</code> pour\nindiquer au client que le contenu n&#39;a pas changé. Celui-ci continuera alors\nd&#39;utiliser sa version locale. Ainsi, on évite de transmettre du contenu inutile\nsur le réseau.</p>\n\n<p>Par contre si le serveur voit que le fichier qu&#39;il possède est plus récent que\nla date envoyée, il répondra avec un <code>200 OK</code> et le nouveau contenu. Ainsi, le\nclient utilise désormais la dernière version. </p>\n\n<p>En faisant ainsi, on évite donc de télécharger un contenu qu&#39;on possède déjà.</p>\n\n<p>Dans les deux cas, le serveur renvoie de nouvelles informations de fraicheur.</p>\n\n<p>Comme pour la fraicheur, il existe deux couples de headers pour communiquer des\ninformations de validation au serveur. En plus de <code>Last-Modified</code>\n/ <code>If-Modified-Since</code> qui utilisent une date de modification, il est possible\nd&#39;utiliser des ETags.</p>\n\n<p>Un ETag est un hash qui identifie de manière unique chaque fichier. Si le\nfichier change, alors son ETag change aussi. Par exemple, le serveur retourne\nau client lors du premier appel un header <code>ETag: &quot;3e86-410-3596fbbc&quot;</code>, et\nlorsque le client fait à nouveau appel à la même ressource, il envoie un header\n<code>If-None-Match : &quot;3e86-410-3596fbbc&quot;</code>. Le serveur va comparer les deux ETags et\nretourner un <code>304 Not Modified</code> s&#39;ils sont identiques ou un <code>200 OK</code> avec le\nnouveau contenu s&#39;ils sont différents.</p>\n\n<p><code>Last-Modified</code> et <code>ETag</code> possèdent des comportements très similaires, mais\nnous vous conseillons d&#39;utiliser <code>Last-Modified</code> en priorité.</p>\n\n<p>En effet, la spec HTTP indique que si un serveur retourne un <code>Last-Modified</code> et\nun <code>ETag</code>, alors le navigateur doit prendre en priorité le <code>Last-Modified</code>. De\nplus, la majorité des serveurs génèrent l&#39;ETag à partir de l&#39;inode du fichier,\nde manière à ce que celui-ci soit modifié au moindre changement.</p>\n\n<p>Malheureusement, ceci pose des soucis pour peu que vous ayez des serveurs\nredondés derrière un load-balancer où chaque serveur possède son propre\nfilesystem et donc ses propres inodes. Deux fichiers identiques, sur deux\nserveurs différents auront des inodes différents et par conséquent des ETag\ndifférents.  Votre système de validation ne fonctionnera plus dès lors que\nvotre client sera redirigé vers un autre frontal.</p>\n\n<p>À noter que ce problème n&#39;apparait pas sous nginx, qui ne prends pas en compte\nl&#39;inode dans la génération de son ETag. Sous Apache, l&#39;option <code>FileEtag MTime\nSize</code> permet de le désactiver, ainsi que <code>etag.use-inode = &quot;disable&quot;</code> sous\nlighttpd.</p>\n\n<h3>Récapitulatif</h3>\n\n<p>À la lumière de ces explications, nous pouvons donc retracer le parcours\nclassique du téléchargement d&#39;un asset mis en cache.</p>\n\n<ul>\n<li>Le client effectue une première requête pour récupérer un asset. Il récupère\nson <code>Cache-Control: max-age</code> pour la fraicheur et son <code>Last-Modified</code> pour la\nvalidation.</li>\n<li>S&#39;il demande à nouveau le même asset alors que celui-ci est encore frais, il\nle prends directement depuis son disque local.</li>\n<li>S&#39;il le demande au dela de sa date de fraicheur, il fait un appel au serveur\nen envoyant son <code>If-Modified-Since</code>.</li>\n<li>Si le fichier sur le serveur possède la même date de modification que celle\nenvoyée, il retourne un <code>304 Not Modified</code>.</li>\n<li>Si le fichier sur le serveur a été modifié, il retourne un <code>200 OK</code> avec le\nnouveau contenu.</li>\n<li>Dans tous les cas, le serveur retourne un <code>Cache-Control</code> et un\n<code>Last-Modified</code>.</li>\n</ul>\n\n<h3>Invalidation du cache</h3>\n\n<p>Mais le cache est un animal capricieux, et nous savons tous que :</p>\n\n<blockquote>\n<p>Il y a deux choses complexes en informatique : invalider le cache et nommer\nles choses.</p>\n</blockquote>\n\n<p>Et effectivement, invalider le cache de nos clients quand nous avons besoin de\nfaire une mise à jour est extrêmement difficile. C&#39;est en fait tellement\ndifficile que nous n&#39;allons pas le faire du tout.</p>\n\n<p>Comme le navigateur mets en cache chaque URL, si nous souhaitons modifier un\ncontenu, il nous suffit de modifier son URL. Et les URL, c&#39;est quelque chose\nque nous avons en quantité illimité. Il nous suffit de modifier le nom d&#39;un\nfichier pour générer un nouvelle URL. On peut ajouter un numero de version,\nun timestamp ou un hash à notre nom de fichier original pour lui générer une\nnouvelle url. </p>\n\n<p>Par exemple : <code>style-c9b5fd6520f5ab77dd823b1b2c81ff9c461b1374.css</code> au lieu de\n<code>style.css</code>.</p>\n\n<p>En mettant un cache très long sur ces assets (1 an est le maximum officiel de\nla spec), c&#39;est comme si on les gardait en cache indéfiniment. Il nous suffit\njuste de mettre un cache plus court sur le fichier qui les référence\n(généralement le fichier HTML).</p>\n\n<p>Ainsi, si on pousse en production une modification sur une feuille de style ou\ndans un script, il nous suffit de modifier les références à ces fichiers dans\nnos sources HTML pour que les clients téléchargent les nouveaux contenus. Le\ncache sur les fichiers HTML est beaucoup plus court, de manière à ce que les\nchangements introduits par notre mise en production soient rapidement\nrépércutées sur nos clients.</p>\n\n<p>Les anciens contenus seront encore en cache chez nos clients mais cela n&#39;a pas\nd&#39;importance, nous ne les requêterons plus jamais et les éléments non-utilisés\ndu cache des clients se vident régulièrement.</p>\n\n<p>La technique est en fait très proche des <code>Etag</code> vus précédement à la différence\nqu&#39;ici nous sommes maitres de la génération du nom unique de fichier et du\nmoment où nous souhaitons invalider le cache de nos clients.</p>\n\n<p>Au final, nous utilisons un mélange de ces deux techniques pour gérer un cache\noptimal. </p>\n\n<p>Les éléments dont l&#39;URL est significative, comme les pages HTML ou les\nretours d&#39;une API définiront une fraicheur faible (de quelques minutes\nà quelques heures, en fonction de la fréquence moyenne de mise à jour). Ceci\npermet de s&#39;assurer que le client aura rapidement la nouvelle version quand\ncelle-ci est déployée, tout en limitant la charge sur le serveur et la quantité\nd&#39;information transitant sur le réseau.</p>\n\n<p>Pour les éléments dont l&#39;URL n&#39;est pas significative, comme les feuilles de\nstyles, les scripts, les polices de caractère ou les images, on utilisera une\nfraicheur maximum d&#39;un an. Ceci permettra au client de garder indéfiniment la\nressource dans son cache sans avoir besoin d&#39;interroger à nouveau le serveur.\nOn générera par contre une URL différente en fonction d&#39;un hash du contenu\nà chaque fois que le contenu vient à changer. On prendra bien garde à modifier\nles références à ces fichiers dans les pages HTML.</p>\n\n<h2>Conclusion</h2>\n\n<p>Nous avons donc vu comment trois points très simples permettent de diminuer\ngrandement le nombre de total de fichiers à télécharger, les rendre plus\nlégers, et les télécharger moins souvent.</p>\n\n<p>La concaténation automatique des fichiers doit être intégrée dans votre\nprocessus de build, afin de garder un environnement de développement clair. La\ncompression en gzip ne nécessite que quelques modifications sur vos serveurs.\nLa mise en place d&#39;une stratégie de cache optimale par contre nécessite à la\nfois des modifications sur le processus de build et sur la configuration des\nserveurs.</p>\n\n<p>Toutes ces modifications sont relativement peu couteuses à mettre en place et\nne dépendent aucunement ni de la technologie utilisée pour le front-end, ni de\ncelle utilisée pour le back-end. Elles peuvent être mise en place quelle que\nsoit votre stack technique. Il n&#39;y a donc plus aucune raison pour ne pas les\ndéployer dès aujourd&#39;hui.</p>\n"
  ],
  "title": "#webperf",
  "tags": [

  ],
  "categories": [

  ],
  "slug": "index",
  "type": "tagindexpage",
  "url": "/tags/webperf/"
}