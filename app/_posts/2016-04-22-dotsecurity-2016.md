---
layout: post
title: "dotSecurity 2016"
tags: dotsecurity
---

First session of the [dotSecurity][1] conference this year. I've been to a lot
of [dotConferences][2] in the past years, and they always took place in the most
gorgeous venues. This one was no exception. As usual, their is also no wifi in
the theater, and attendees are invited to not use their laptops.

Obviously, I did not follow that advice, otherwise you couldn't have been able
to read this short recap.

# Making HTTPS accessible

The first talk of the day was by [Joona Hoikkala][3], about https, and the
hassle it is to properly install and configure it to a server. The overhead is
getting so low and the benefits so high in term of security and privacy, that
the whole web should be https. It protects end-users from session hijacking,
content injection and arbitrary censorship.

![Joona](/img/2016-04-22/https.jpg)

And yes, I get the irony that, at the time of writing, this very own website is
not served through https. I'll fix that soon.

The other advantages are that http/2 will only work with https. Well, according
to the specs, it could also run without it. But in
practice, no current implementation handle this case, and all need https
enabled. It is also said that it should give better SEO results, but as with all
things SEO-related, I wouldn't put much faith in that.

Still, the issue today (and that will be my excuse for not having https on this
very own website) is that it is hard and cumbersome (and expensive!) to get
a certificate for each server we own. We could default to a wildcard server
(something like `*.pixelastic.com`), but this will just lower the overall
security because if one server got compromised, all are insecure.

The solution today is to use [Let's Encrypt][4] that gives easy and free
certificates for everybody. It comes with a command line interface and ease the
burden of creating, revoking and updating certificates. There are more than
2 millions certificates issued through Let's Encrypt since its creation.

Overall, I must say that this first talk was a bit slow to start. I took me
a while to understand that the speaker actually worked for Let's Encrypt. If
anything, it gives me one more nudge to force me to move my servers to https.

# Life of a vulnerability

The second talk was by [Filippo Valsorda][5], from [Cloudflare][6]. He explained
in very simple terms how vulnerabilities (or vulns) are discovered, published
and fixed.  Really nice overview for something that I only hear from a distance
without knowing how the process really works.

![Filippo](/img/2016-04-22/vuln.jpg)

He started by defining a vuln by being basically a state of software that lets
user do something that they are not allowed to do. Most common forms include
DDOS, leaks of data, SQL injections or remote code execution.

Not all vulns have a nice name, website, logo and stickers. Most of them don't,
actually. When you discover a vulnerability, you have to report it. There are
two ways to report them: full disclosure and responsible disclosure.

For full disclosure, you post the vulnerability to a public forum, letting
everybody know about it. Such forums are often target of legal threats of
companies that don't want their vulnerabilities publicly accessible. Other
people can choose to sell them. Who actually buy this vulns is still a bit
obscure for me, but it can be people buying them to resell them later, or use
them for their own profit (either governments or mafias).

For responsible disclosure (also known as coordinated disclosure), you do not
post publicly, but contact the website/owner of the service being impacted. You
tell them about the issue, the risks and how to fix it. This gives them the time
to fix the issue before it does too much damage. That is why it is very
important on any website or open-source software to have a way to contact the
team for security reason. It can be a simple security@domain.com email address,
but it shows that the team will take the matter seriously.

Also, being publicly acknowledged has having found a vuln on major websites like
Google or Facebook is very nice for exposure and building a reputation of
security expert.

Once the vuln is identified, it should be assigned a unique number so everybody
knows they are talking about the same issue, and can work in a coordinated
effort to fix it. There are hundreds of vulnerabilities issued per
month, some will target only a very specific version of a very specific
framework while other can have a much bigger impact.

Such identifies vulns are publicly announced with the list of affected version
of the software, the explanation of the issue and the real world impact of such
issue. Alongside are displayed patches and/or possible fixes as well as credits.

Then, he ended his presentation with something I found a bit weird to do in such
a conference. He did a live exploit of a known vulnerability on an old version
of Ruby on Rails to allow remote code execution. He used MetaSploit and Shodan
to perfectly show how easy it is to find an insecure machine and run a known
exploit to it.

Sure, the machine he targeted was his own, but it wasn't really clear at first.
I don't think it was such a good idea to show the audience how easy it is to get
access to distant machine and start breaking them, without telling them about
the legal implication of doing (like it could get you in jail in some
countries). I would have appreciated a bit more context here.

Still, very good talk that taught me a lot.

# Multi-factor authentication

Following talk was by [Jacob Kaplan-Moss][7]. He told us everything about
multi-factor authentication (which is a fancy word to say two-factor
authentication, I didn't get the difference).

![Jacob](/img/2016-04-22/2fa.jpg)

Login to a website simply through a password is no longer enough. We need
a secondary channel to confirm the person doing the action is actually the one
we think it is. Computers with password saved in cache can be stolen, or left
unlocked.

For 2FA, we need another object that is owned by the user trying to do
a sensitive action. Most of the time it is mobile phone with a 2FA application
that display a small code that you have to input on the website, but there are
other ways.

You could send a SMS to the user, or give him an automated phone call with
a special number he need to input. This is called _out of band communication_,
you request information from the user in channel A that was send to him through
channel B. This way you make sure you're not discussing with a fake user
impersonating the real one because he stole his laptop.

There are two type of tokens you can exchange that way. Soft tokens are the own
that are generated through 2FA Apps like [Google Authenticator][8] or
[Authy][9]. Hard tokens are physical objects like [YubiKeys][10].

All those solutions offer various degrees of risks, UX and cost. When choosing
a solution, you should not only focus on the risks. If you take only the less
risky solution you often end-up with the most miserable of UX and users will in
the end not use your ultra-secure solution because it just too damn hard to use.

When assessing risk, you have to check if the token can be intercepted, if they
can be brute-forced, if you can notice if a token has been stolen. You also have
to check how the apps or hardware is protected against malware.

When assessing costs, you have to take into account both the cost for the
company (how much does it cost to give YubiKeys to everybody?) as well as the
cost for the end-user (will it be cumbersome, will they lose time doing it,
etc.).

For example an out of band communication through SMS is easy. Most of your users
already have a phone, and they know how to use SMS. It is also quite easy to
intercept SMS, and such a communication channel could easily fail in countries
where the phone network coverage is bad.

For soft tokens through authentication apps, it is much harder to compromise
tokens (even if it possible), but you don't need a network as it can work
offline. Still, there is a UX cost because you need to download an external app
and manually input the code from your phone.

For hard tokens, the real threat is that the master key that can validate the
tokens for all physical keys can be stolen. There is no risk-free solution, and
even if such an event is unlikely, if this happens, you need to build new keys
for everybody and ship them to wherever they are in the world. Cost-wise, it can
become really expensive. But in term of UX, this is the must, you just have to
press a button to prove that you are you.

Whatever the method you chose, you have to ask for 2FA not only when they login,
but whenever they request a sensitive action. This could be adding new members
on a team, changing passwords or billing information. You should also monitor
for weird behavior, like changes in connecting devices or geo-location.

You also need a backup plan. What happens when people lose their phone or keys.
How can they connect to the service again? There is no perfect answer for that
question. You could provide user with backup recovery code, but then the user
must store them securely somewhere which just moves the burden of security to
the average users. Most won't store them at all.

You could allow for a backup phone, but this only increase the attack surface,
making everything less secure.  You could allow the support team to help, but
they will then become open to social engineering. Or you can simply state that
there is no way to recover if you lost your authentication method. Whatever
method you chose, you have to tell you users upfront or they will be really
pissed off when they discover they have no way to get their account back.

The speaker suggestion was the following setup, that differentiate between
public accounts and internal accounts:

For __public accounts__, 2FA apps are the best trade-off in terms of UX and
security. Authy being better than Google Authenticator UX-wise, but more costly
for the company. You also have to request the token on each sensitive action.
For recovery, don't allow support to recover an account, and provide backup
codes.

For __internal accounts__, you can greatly improve both security and UX by putting
a bit more money on the table and giving a YubiKey to each employee for 2FA.
This will prevent outsiders from taking over an employee account. Add behavior
analysis, like detecting when a connection is not done from the known
workplaces, and trigger a 2FA check. Also, only allow 2FA reset when asked face
to face to the security team.

I really liked a quote of the speaker during the short Q&A session that came
afterwards:

> Security should not be handled by a security team. It should be for the day to
> day developer. Just like tests should no longer be in the hand of testing
> teams but embedded into development through things like TDD.

# Lightning talks

After those talks and a break, we continued with a small commercial break, aka.
Lightning talks.

The first one presented in the form of a git branch workflow what he called the
classical "security audit workflow". You request a security audit on your code,
so the security team creates a new `secuirty` branch to start working on it.
During that time, the main `develop` branch keeps growing because devs don't
stop coding during the audit. Then it's time for the release, so we merge
`develop` into `release`. But the security audit wasn't finished, so no security
fixed was pushed to production. And once the security audit is done, the code on
`develop` has changed so much that the audit is useless.

His solution to that was to do runtime checking of the code, through his company
called [Sqreen][11].

From my point of view, the whole premises where fucked up. He was giving
a solution to the wrong problem. Why would you even start a security audit if
you're not going to implement the results before shipping? This is the root
cause of the issue. Either integrate security testing in the flow process, fix
issues before release or open a public bounty program. There are so many other
ways to avoid the absurd previous branching model shown on screen.

I'm not saying Sqreen product is bad (we even used it at some point), but that
what it was trying to solve was not a tech issue but an organizational issue.

Then [Ori][12] did some Ori on stage, showing dead squirrels and demonstrating
a slack bot to publicly shame `sudo` users.

Finally a guy from [OpenCredo][13] tried to explain something, but as he was
constantly looking at the slides behind him, I couldn't hear what he was saying.

I'm a bit disappointed by the lightning talks of this edition. This looks too
much like the bad sessions of the [ParisTechTalk][14] meetup, where you have
talks that are only an excuse to sell a product or company. This is especially
surprising because all the other talks of all the other dotConferences takes
great care to avoid talking about their companies.

# Content Security Policy

We then came back to the real talks with [Scott Helme][15] and the CSP (Content
Security Policy). I discovered the CSP last October at [ParisWeb][16], so
nothing really new here. I still think it's something I should use more and it
got me thinking about so many usages.

![Scott](/img/2016-04-22/csp.jpg)

But first, let's explain what that is. Basically it's a new header
(`content-security-policy`), that you add to your responses. Its value is a long
string.

It will tell the browser what it can or can't do regarding loading of external
sources. And the browser support is actually quite good already. You can tell
the browser to only load assets from your specific CDN as well as your main
domain, to protect you from XSS. You can fine-tune it to apply different rules
for images, videos, scripts, styles, etc. You can prevent your page from being
embedded in an iframe, etc.

By default it will block the execution of all inline `script` in your page,
forcing you to load them from an external file that should be hosted on one of
the servers in the whitelist. Another cool feature is to automatically disable
the loading of `http` assets on an `https` page, avoiding the warning about
mixed content. You can even force the browser to always upgrade `http` urls to
`https`.

But the two most interesting meta features are that you can toggle a report mode
that will not block anything, but only display warning in the console. This lets
you test it to see what will break before really deploying it. The other feature
is a way to send those reports to an external server instead of displaying them
in the console, for later analysis.

I wonder how easy it is to test those CSP with statically generated websites
(the Middleman/Jekyll type). I'm also wondering how much we could exploit this
mechanism to send detailed information about the user, directly from the
browser, without any third-party tracking script.

I mean, if I turn on report mode only and disable loading of all assets, I will
receive the report of all the assets the user wanted to load. Doing so I'll be
able to analyze for example which users have an adblocker (because they didn't
got "blocked" while trying to load an ad). Could I also load a script from
a valid source that will dynamically inject a custom script with all the user
data (viewport, ip, user-agent, etc.) in its url to log it?

I'm sure there are really interesting data to get from this feature, as well as
increase the overall security of apps of course!

# Diogo Monica

The next talk was my personal favorite of the whole afternoon. It was made by
[Diogo Monica][17], head of security at [Docker][18]. I did not get everything
from the talk to be honest, but it was explained in such a clear and concise way
that I really liked it.

![Diogo](/img/2016-04-22/package.jpg)

At its core, Diogo said that updates were one of the most, if not the most,
important parts of security. When an issue is found and fixed, you have to
update your software to actually enjoy this fix. And how do you update? Through
your package manager.

So package manager are actually a very important part of the security chain, and
any security issue in the package manager itself can compromise any package it
handles.

All package managers today download updates through a secure connection. This
offer great protection against man-in-the-middle (MITM) attacks, but is not
enough. If the source is compromised, you are still downloading an infected
update through a secure channel.

The best way to protect sources is to sign them, usually through PGP. As
a maintainer, you add your PGP "stamp" on the package, and you give your public
key to anybody wanting to check your packages. When a user downloads the
package, he just have to check (with the public key), that the package was
actually packaged by the maintainer. If an attacker changes anything in the
source, the key check will fail.

But this won't protect you against what is named a "downgrade attack". The PGP
signature only lets you check that the package you've download actually comes
from the maintainer.

Now imagine you're using a specific piece of software in v1. You've downloaded
the binary from the source, checked that it's really coming from the official
source and install it. Two weeks later, a vuln is found in this version. A v2
is released that fixes the issue. You install it through your package manager,
it downloads it from the source, checks that it really comes from the
maintainer, and installs it. You're safe!

Or maybe not. It only checked that the file downloaded came from the maintainer.
Not that it actually was the v2. Maybe you just reinstalled the exact same
software. Maybe one of the servers that offered the download was compromised and
offered v2 instead of v1 and you didn't see it because the PGP protection
doesn't check the version, only that it comes from the correct maintainer.

An easy way to check would be to have a look at your current installed version.
It's quite easy, but it is not the job of PGP, and it is apparently quite
difficult for package manager to check that the correct version is installed.

Diogo then talked about the way to delegate power through a hierarchy of keys.
Having one master key (stored offline), that could delegate power to other keys
that would themselves delegate power. Keys on the top of the hierarchy would
have longer expiration time while keys at the bottom will have short expiration
time.

He also talked about a way to sign a package with multiple keys, to validate for
example that a specific binary has correctly passed the staging and preprod
tests. I must say I did not get exactly how all this part was
working, but it is all included in the way Docker does manage its packages.

Really nice talk, I strongly encourage you to watch the video once it will be
available.

# Anne Canteaut

Next talk was not a very technical one, but more mathematical oriented. It was
done by a researcher on cryptography. She pointed out that most of the security
breaches described in the previous talks were never about weakness found in
cryptographic parts.

![Anne](/img/2016-04-22/crypto.jpg)

Does that mean that the crypto part of security is perfect? Far from it. If you
listen to crypto analysts, seems like a lot of algorithms are breached on
a regular basis. But what does that mean anyway? And should we worry?
Cryptographers are known to be paranoid, so should we really listen to them?

Spoiler: yes.

If cryptographers says that something is broken, you should stop using right
now. Even if there is no practical way to exploit the weakness today, there will
be in the future. Those breaches never get better, they always get worse.

There are two ways to consider a hash algorithm to be breached. The first one is
to find two inputs that gives the same hash output. This is called a collision
and `md5` is breached in that way. This is "quite easy" to do, but does not
really reduce the security level in any significant way. It opens the way to
other exploits, but is not too dangerous in itself.

The second way is far more dangerous. It is when you are able to find
a preimage, this means when given a specific hashed output, you can craft your
own input that will result in this output. This is slightly different from the
collision in one important way: the output is arbitrary and already given to
you. It's like the birthday paradox: it is easier in any group of person to find
two person with the same birthday, than in the same group finding one with the
same birthday as you.

If you know how to craft a preimage, you will then be able to create an infected
version of an valid input, that will give the same output as the valid one. If
this happens, the algo is breached, and you should stop using it.

The number one rule of cryptography is to always trust public analysis. A lot of
people can come up with new algorithm to hash data in a secure way. But it
requires a large number of eyes looking at how it's done and brains trying to
find breaches to actually prove that one is secure or not.

One of the most famous, AES, has been initially published in 1998 and is still
used today, without any breach. The explanation of why it is so resilient is
because he is the winner of a publicly held competition of algorithm that took
place for 5 years. The best way to make your algo win in such a competition is to
find breaches in the algo of the others. Doing so, you're sure to have the
brightest minds on the subject trying to breach into it.

It was nice to have a glimpse of the mathematical side of this world. The
subject is still very far from what I do, but interesting. I am completely
unable to assess if a cryptographic algorithm is good or not, so I trust the
experts on the subject. And it's nice to see that even expert can't easily
assess every algo, and so that they have to trust each other on public analysis.

# Paul Mockapetris

We then had a quick Q&A session with Paul Mockapetris, inventor of the DNS. From
this exchange I'll only remember the following quote:

![Paul](/img/2016-04-22/paul.jpg)

> Hardware is like milk, you want it the freshest you can find.
> Software is like wine, you want it with a bit of age.

# Web Platform Security

The last talk I could attend was by [Mike West][19]. There was another talk
after this one, but I had a train to catch so couldn't stay up to the end. You
can find the slides [in here][20].

![Mike](/img/2016-04-22/mike.jpg)

Mike started by telling us the story of Ulysses and the sirens. The sirens were
beautiful female creatures with charming and dangerous voices. If men hear
them, they would become crazy and jump overboard and drown. Ulysses learned from
Circe that the only way to protect against the siren singing was to either put
wax in your ears, or to tie yourself to the mast.

Same goes in web security. You have to tie yourself to the mast, meaning you
have to follow a principle of least privilege. That way, even if an attacker
manages to take control of your account, he won't be able to do much. It is
better to do it that way than to try to block every possible way an attacker
could compromise an account.

It is especially true for browsers. Browsers are becoming more and more
powerful, accessing banking websites, connecting to bluetooth and camera. And
being used by average users everyday. Mike, working in the Chrome dev team, told
us about the beta features they are working on in Chrome.

He told us about the CSP, as we've seen, and its limitations. If you have
different applications running on different subdomains, you cannot whitelist the
main domain, because it will expose all subdomains to a breach in any of them.
You could host a malicious script in one infected subdomain and make it load in
another. They are working on something that would allow better granularity on
the `scheme`, `host` and `port` allowed.

They are also adding another way to check for loaded script by adding a checksum
verification on script. You would call your `<script integrity="{hash}"
src="...">` and the browser will check that the checksum of the loaded file
matches the specified hash before executing it.

# Conclusion

Really nice first occurrence of dotSecurity. Another one will take place next
year and I will surely go again. I'll try to be an ambassador this time as well.

The panel of talks was large, from generic knowledge about the security world,
to more technical one. I would say the mix was perfect.

Next year I'd like to see talks explaining in more details the selling and
buying of vulnerabilities. Who discover them? How? Why? Who do they sell them
to? How much? What do buyer do with it, etc.

[1]: http://www.dotsecurity.io/
[2]: http://www.dotconferences.eu/
[3]: https://twitter.com/joohoi
[4]: https://letsencrypt.org/
[5]: https://twitter.com/FiloSottile
[6]: https://www.cloudflare.com/
[7]: https://twitter.com/jacobian
[8]: https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2&hl=en
[9]: https://www.authy.com/
[10]: https://www.yubico.com/products/yubikey-hardware/
[11]: https://www.sqreen.io/
[12]: https://twitter.com/oripekelman
[13]: https://opencredo.com/
[14]: http://www.meetup.com/fr-FR/Paris-Tech-Talks/?chapter_analytics_code=UA-49948924-1
[15]: https://twitter.com/Scott_Helme
[16]: http://www.paris-web.fr/2015/conferences/csp-content-security-policy.php
[17]: https://twitter.com/diogomonica
[18]: https://www.docker.com/
[19]: https://twitter.com/mikewest
[20]: https://speakerdeck.com/mikewest/web-platform-security-dotsecurity-april-2016
